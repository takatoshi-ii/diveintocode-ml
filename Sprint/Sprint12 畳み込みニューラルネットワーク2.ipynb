{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 深層学習スクラッチ 畳み込みニューラルネットワーク2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprintの目的\n",
    "スクラッチを通してCNNの基礎を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "スクラッチで2次元用畳み込みニューラルネットワークを実装した後、学習と検証を行なっていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2次元の畳み込みニューラルネットワークスクラッチ\n",
    "\n",
    "2次元に対応した畳み込みニューラルネットワーク（CNN）のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "プーリング層なども作成することで、CNNの基本形を完成させます。クラスの名前はScratch2dCNNClassifierとしてください。\n",
    "\n",
    "\n",
    "### データセットの用意\n",
    "引き続きMNISTデータセットを使用します。2次元畳み込み層へは、28×28の状態で入力します。\n",
    "\n",
    "今回は白黒画像ですからチャンネルは1つしかありませんが、チャンネル方向の軸は用意しておく必要があります。\n",
    "\n",
    "(n_samples, n_channels, height, width)のNCHWまたは(n_samples, height, width, n_channels)のNHWCどちらかの形にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】2次元畳み込み層の作成\n",
    "\n",
    "1次元畳み込み層のクラスConv1dを発展させ、2次元畳み込み層のクラスConv2dを作成してください。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$\n",
    "a_{i,j,m} = \\sum_{k=0}^{K-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1}x_{(i+s),(j+t),k}w_{s,t,k,m}+b_{m}\n",
    "$$\n",
    "$a_{i,j,m}$ : 出力される配列のi行j列、mチャンネルの値\n",
    "\n",
    "\n",
    "$i$ : 配列の行方向のインデックス\n",
    "\n",
    "\n",
    "$j$ : 配列の列方向のインデックス\n",
    "\n",
    "\n",
    "$m$ : 出力チャンネルのインデックス\n",
    "\n",
    "\n",
    "$K$ : 入力チャンネル数\n",
    "\n",
    "\n",
    "$F_{h}, F_{w}$ : 高さ方向（h）と幅方向（w）のフィルタのサイズ\n",
    "\n",
    "\n",
    "$x_{(i+s),(j+t),k}$ : 入力の配列の(i+s)行(j+t)列、kチャンネルの値\n",
    "\n",
    "\n",
    "$w_{s,t,k,m}$ : 重みの配列のs行t列目。kチャンネルの入力に対して、mチャンネルへ出力する重み\n",
    "\n",
    "\n",
    "$b_m$ : mチャンネルへの出力のバイアス項\n",
    "\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "\n",
    "次に更新式です。1次元畳み込み層や全結合層と同じ形です。\n",
    "\n",
    "$$\n",
    "w_{s,t,k,m}^{\\prime} = w_{s,t,k,m} - \\alpha \\frac{\\partial L}{\\partial w_{s,t,k,m}} \\\\\n",
    "b_{m}^{\\prime} = b_{m} - \\alpha \\frac{\\partial L}{\\partial b_{m}}\n",
    "$$\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{s,t,k,m}} : w_{s,t,k,m} に関する損失 L の勾配\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b_{m}}$ : $b_{m}$ に関する損失 L の勾配\n",
    "\n",
    "\n",
    "勾配 $\\frac{\\partial L}{\\partial w_{s,t,k,m}}$ や $\\frac{\\partial L}{\\partial b_{m}}$ を求めるためのバックプロパゲーションの数式が以下である。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{s,t,k,m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1} \\frac{\\partial L}{\\partial a_{i,j,m}}x_{(i+s)(j+t),k}\\\\\n",
    "\\frac{\\partial L}{\\partial b_{m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1}\\frac{\\partial L}{\\partial a_{i,j,m}}\n",
    "$$\n",
    "$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi行j列、mチャンネルの値\n",
    "\n",
    "\n",
    "$N_{out,h},N_{out,w}$ : 高さ方向（h）と幅方向（w）の出力のサイズ\n",
    "\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{i,j,k}} = \\sum_{m=0}^{M-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1} \\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}}w_{s,t,k,m}\n",
    "$$\n",
    "$\\frac{\\partial L}{\\partial x_{i,j,k}}$ : 前の層に流す誤差の配列のi列j行、kチャンネルの値\n",
    "\n",
    "\n",
    "M : 出力チャンネル数\n",
    "\n",
    "\n",
    "ただし、 $i-s<0$ または $i-s>N_{out,h}-1$ または $j-t<0$ または $j-t>N_{out,w}-1$ のとき $\\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}} =0$ です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜SGD＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        for key in ('W','B'):\n",
    "            layer.network[key] -= self.lr * layer.grad[key] \n",
    "        \n",
    "        return layer\n",
    "\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr        \n",
    "        self.h = {}\n",
    "        self.initial = True\n",
    "        self.log_flg = True\n",
    "        self.i_cnt = 0\n",
    "    \n",
    "    def update(self, layer):\n",
    "\n",
    "        if self.initial == True:\n",
    "            print(\"self.h is None\")\n",
    "            self.h = {}\n",
    "            self.h[\"W\"] = np.zeros_like(layer.network[\"W\"])    \n",
    "            self.h[\"B\"] = np.zeros_like(layer.network[\"B\"])     \n",
    "            self.initial = False\n",
    "                    \n",
    "        for key in (layer.network.keys()):\n",
    "            self.h[key] += np.square(layer.grad[key])\n",
    "            layer.network[key] -= self.lr * layer.grad[key] / (np.sqrt(self.h[key]) + 1e-7)  \n",
    "            \n",
    "        return layer\n",
    "    \n",
    "\n",
    "class Adam:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr        \n",
    "        self.h = {}\n",
    "        self.initial = True\n",
    "        self.log_flg = True\n",
    "        self.i_cnt = 0\n",
    "    \n",
    "    def update(self, layer):\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999 \n",
    "        if self.initial == True:\n",
    "            print(\"self.h initial\")\n",
    "            self.m = {}\n",
    "            self.v = {}\n",
    "            for key in (layer.network.keys()):\n",
    "                self.m[key] = np.zeros_like(layer.network[key])    \n",
    "                self.v[key] = np.zeros_like(layer.network[key]) \n",
    "\n",
    "            self.initial = False\n",
    "\n",
    "        learning_rate_t  = self.lr * np.sqrt(1.0 - beta2 ** (self.i_cnt + 1)) / (1.0 - beta1 ** (self.i_cnt + 1))    \n",
    "\n",
    "        for key in (layer.network.keys()):\n",
    "            self.m[key] += (1 - beta1) * (layer.grad[key] - self.m[key])\n",
    "            self.v[key] += (1 - beta2) * (layer.grad[key] ** 2 - self.v[key])            \n",
    "            layer.network[key] -= learning_rate_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)      \n",
    "\n",
    "        self.i_cnt += 1\n",
    "            \n",
    "        return layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜function＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class function:\n",
    "    def im2col(self, input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "        N, C, H, W = input_data.shape\n",
    "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "        col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "        for y in range(filter_h):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(filter_w):\n",
    "                x_max = x + stride*out_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "        return col\n",
    "\n",
    "\n",
    "    def col2im(self, col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "        N, C, H, W = input_shape\n",
    "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "        col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "        img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "        for y in range(filter_h):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(filter_w):\n",
    "                x_max = x + stride*out_w\n",
    "                img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "        return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜GetMiniBatch＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜loss_function＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_function:\n",
    "# クロスエントロピー\n",
    "    def cross_entropy_error(self, d, y):\n",
    "        if y.ndim == 1:\n",
    "            d = d.reshape(1, d.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "\n",
    "        # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "        if d.size == y.size:\n",
    "            d = d.argmax(axis=1)\n",
    "\n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜Conv2d＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\"]\n",
    "    \"\"\"\n",
    "    def __init__(self, F_cnt, F_ch, F_size, optimizer, stride = 1, pad = 0):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        self.network = {}\n",
    "        #self.network[\"W\"] = initializer.W(F_cnt, F_ch, F_h, F_w)\n",
    "        #self.network[\"B\"] = initializer.B(F_cnt)\n",
    "        \n",
    "        # フィルターサイズは縦横同じ(F_size)とする。\n",
    "        self.network[\"W\"] = np.random.randn(F_cnt, F_ch, F_size, F_size)\n",
    "        self.network[\"B\"] = np.random.randn(F_cnt)\n",
    "\n",
    "        self.grad = {}\n",
    "        self.X = 0\n",
    "        #self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.func = function()\n",
    "\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "    #def forward(self, X, W, B):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"  \n",
    "        self.X = X  \n",
    "        \n",
    "        FN, FC, FH, FW = self.network[\"W\"].shape\n",
    "        \n",
    "        N, XC, XH, XW = X.shape\n",
    "        \n",
    "        \n",
    "        # 出力値のheight, width\n",
    "        out_h = self._N_out(XH, self.pad, FH, self.stride)\n",
    "        out_w = self._N_out(XW, self.pad, FW, self.stride)\n",
    "\n",
    "        self.func = function()\n",
    "        \n",
    "        # xを行列に変換\n",
    "        col = self.func.im2col(X, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        # フィルターをxに合わせた行列に変換\n",
    "        col_W = self.network[\"W\"].reshape(FN, -1).T\n",
    "\n",
    "        \n",
    "        \n",
    "        # 順伝播\n",
    "        A = np.dot(col, col_W) + self.network[\"B\"]\n",
    "    \n",
    "        # 形状を元に戻す。(バッチサイズ、出力高さ、出力幅、)\n",
    "        test = A.reshape(N, int(out_h), int(out_w), -1)\n",
    "        A = A.reshape(N, int(out_h), int(out_w), -1).transpose(0, 3, 1, 2)\n",
    "        self.A = A    \n",
    "        \n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return self.A\n",
    "        \n",
    "\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "\n",
    "        FN, FC, FH, FW = self.network[\"W\"].shape\n",
    "        \n",
    "        dA = dA.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "\n",
    "        \n",
    "        self.grad = {}\n",
    "        # bの勾配        \n",
    "        self.grad[\"B\"] = np.sum(dA, axis = 0) \n",
    "        # Wの勾配   \n",
    "        self.grad[\"W\"] = np.dot(self.col.T, dA)\n",
    "        self.grad[\"W\"] = self.grad[\"W\"].transpose(1, 0).reshape(FN, FC, FH, FW)\n",
    "        \n",
    "        # dxをim2colに通したdcolを計算\n",
    "        dcol = np.dot(dA, self.col_W.T)\n",
    "        \n",
    "        # doutをdxに変換\n",
    "        dx = self.func.col2im(dcol, self.X.shape, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "\n",
    "        return dx\n",
    "\n",
    "    \n",
    "    def _N_out(self, input_size, padding, filter_size, straide):\n",
    "        Nout = (input_size + 2*padding - filter_size)/straide + 1\n",
    "\n",
    "        return Nout\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】2次元畳み込み後の出力サイズ\n",
    "\n",
    "畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください。\n",
    "\n",
    "$$\n",
    "N_{h,out} =  \\frac{N_{h,in}+2P_{h}-F_{h}}{S_{h}} + 1\\\\\n",
    "N_{w,out} =  \\frac{N_{w,in}+2P_{w}-F_{w}}{S_{w}} + 1\n",
    "$$\n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$S$ : ストライドのサイズ\n",
    "\n",
    "\n",
    "$h$ が高さ方向、 $w$ が幅方向である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_out(self, input_size, padding, filter_size, straide):\n",
    "    Nout = (input_size + 2*padding - filter_size)/straide + 1\n",
    "    \n",
    "    return Nout\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】最大プーリング層の作成\n",
    "\n",
    "最大プーリング層のクラスMaxPool2Dを作成してください。プーリング層は数式で表さない方が分かりやすい部分もありますが、数式で表すとフォワードプロパゲーションは以下のようになります。\n",
    "\n",
    "$$\n",
    "a_{i,j,k} = \\max_{(p,q)\\in P_{i,j}}x_{p,q,k}\n",
    "$$\n",
    "$P_{i,j}$ : i行j列への出力する場合の入力配列のインデックスの集合。 $S_{h}×S_{w}$ の範囲内の行（p）と列（q）\n",
    "\n",
    "\n",
    "$S_{h}, S_{w}$ : 高さ方向（h）と幅方向（w）のストライドのサイズ\n",
    "\n",
    "\n",
    "$(p,q)\\in P_{i,j}$ : $P_{i,j}$ に含まれる行（p）と列（q）のインデックス\n",
    "\n",
    "\n",
    "$a_{i,j,m}$ : 出力される配列のi行j列、kチャンネルの値\n",
    "\n",
    "\n",
    "$x_{p,q,k}$ : 入力の配列のp行q列、kチャンネルの値\n",
    "\n",
    "\n",
    "ある範囲の中でチャンネル方向の軸は残したまま最大値を計算することになります。\n",
    "\n",
    "\n",
    "バックプロパゲーションのためには、フォワードプロパゲーションのときの最大値のインデックス (p,q) を保持しておく必要があります。フォワード時に最大値を持っていた箇所にそのままの誤差を流し、そこ以外には0を入れるためです。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜MaxPool2D＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "        \n",
    "        self.func = function()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # xを行列に変換\n",
    "        col = self.func.im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        # プーリングのサイズに合わせてリサイズ\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        # 行ごとに最大値を求める\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        # 整形\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        \n",
    "        # 最大値のindexを保持\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = self.func.col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】（アドバンス課題）平均プーリングの作成\n",
    "平均プーリング層のクラスAveragePool2Dを作成してください。\n",
    "\n",
    "\n",
    "範囲内の最大値ではなく、平均値を出力とするプーリング層です。\n",
    "\n",
    "\n",
    "画像認識関係では最大プーリング層が一般的で、平均プーリングはあまり使われません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＜AveragePool2D＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePool2D:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_mean = None\n",
    "        \n",
    "        self.func = function()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # xを行列に変換\n",
    "        col = self.func.im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        # プーリングのサイズに合わせてリサイズ\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        # 行ごとに平均値を求める\n",
    "        arg_mean = np.mean(col, axis=1)\n",
    "        out = arg_mean\n",
    "        # 整形\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        \n",
    "        # 最大値のindexを保持\n",
    "        self.arg_mean = arg_mean\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        # 全面０で初期化\n",
    "        dmean = np.zeros((dout.size, pool_size))\n",
    "        \n",
    "        # 平均値で塗り潰し\n",
    "        dmean[np.arange(self.arg_mean.size), self.arg_mean.flatten()] = dout\n",
    "        \n",
    "        dmean = dmean.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmean.reshape(dmean.shape[0] * dmean.shape[1] * dmean.shape[2], -1)\n",
    "        dx = self.func.col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】平滑化\n",
    "平滑化するためのFlattenクラスを作成してください。\n",
    "\n",
    "フォワードのときはチャンネル、高さ、幅の3次元を1次元にreshapeします。その値は記録しておき、バックワードのときに再びreshapeによって形を戻します。\n",
    "\n",
    "この平滑化のクラスを挟むことで出力前の全結合層に適した配列を作ることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜Flattern＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flattern:\n",
    "    def __init__(self):\n",
    "        self.N = 1\n",
    "        self.C = 1\n",
    "        self.H = 1\n",
    "        self.W = 1\n",
    "        \n",
    "        \n",
    "    def forward(self, X_in):\n",
    "        self.N, self.C, self.H, self.W = X_in.shape\n",
    "        X_out = X_in.reshape(self.N, self.C * self.H * self.W)\n",
    "        return X_out\n",
    "    \n",
    "    def backward(self, X_out):\n",
    "        N, Flat = X_out.shape\n",
    "        X_in = X_out.reshape(N, self.C, self.H, self.W)\n",
    "        return X_in\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(\n",
    "    [[ 1., 2., 3.],\n",
    "       [ 4., 5., 6.],\n",
    "       [ 7., 8., 9.]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[2., 1.],\n",
       "         [0., 9.]],\n",
       "\n",
       "        [[9., 7.],\n",
       "         [9., 1.]]],\n",
       "\n",
       "\n",
       "       [[[6., 3.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[1., 6.],\n",
       "         [3., 1.]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "a = np.random.rand(2,2,2,2) * 10\n",
    "a = np.floor(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1., 0., 9., 9., 7., 9., 1.],\n",
       "       [6., 3., 0., 0., 1., 6., 3., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat = clf_Flattern()\n",
    "flat.forward(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここまでをクラス化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    活性化関数：sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : 活性化関数への入力\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.Z = 0\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.Z = 1/(1 + np.exp(-A))\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dx = dZ * (1.0 - self.Z) * self.Z\n",
    "        return dx\n",
    "\n",
    "    \n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    活性化関数：sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : 活性化関数への入力\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.A = 0\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dx = dZ * (1.0 - self.forward(self.A)**2)\n",
    "        return dx\n",
    "    \n",
    "\n",
    "\n",
    "class Relu:\n",
    "    \"\"\"\n",
    "    活性化関数：relu\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : 活性化関数への入力\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.A = 0\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.maximum(0, A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, Z):\n",
    "        dx = np.where(self.A > 0, Z, 0)\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    活性化関数：softmax\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : 活性化関数への入力\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):\n",
    "        if A.ndim == 2:\n",
    "            A = A.T\n",
    "            A = A - np.max(A, axis=0)\n",
    "            y = np.exp(A) / np.sum(np.exp(A), axis=0)\n",
    "            return y.T\n",
    "\n",
    "        A = A - np.max(A) # オーバーフロー対策\n",
    "        return np.exp(A) / np.sum(np.exp(A))\n",
    "    \n",
    "    def backward(self, y, y_pred):\n",
    "        batch_size = y.shape[0]   \n",
    "        if y.size == y_pred.size: # 教師データがone-hot-vectorの場合\n",
    "            dx = (y_pred - y) / batch_size\n",
    "        else:\n",
    "            dx = y.copy()\n",
    "            dx[np.arange(batch_size), y] -= 1\n",
    "            dx = dx / batch_size\n",
    "            \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ＜SimpleInitializer＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        #print(\"W.shape = \", W.shape)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "\n",
    "        B = np.zeros(n_nodes2)\n",
    "\n",
    "        return B\n",
    "    \n",
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        print(\"★★★XavierInitializer\")\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        print(\"★★★XavierInitializer: W Set\")\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.W = np.random.rand(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)     \n",
    "\n",
    "        return self.W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        print(\"★★★XavierInitializer: B Set\")\n",
    "        self.B = np.random.randn(n_nodes2) \n",
    "        \n",
    "        return self.B\n",
    "    \n",
    "    \n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = np.random.rand(n_nodes1, n_nodes2) * np.sqrt(2 /n_nodes1)\n",
    "\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.random.randn(n_nodes2)\n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜FC＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        self.network = {}\n",
    "        self.network[\"W\"] = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.network[\"B\"] = initializer.B(n_nodes2)\n",
    "        self.A = 0\n",
    "        self.grad = {}\n",
    "        self.X = 0\n",
    "\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        # 自層への入力\n",
    "        A =  np.dot(X, self.network[\"W\"] ) + self.network[\"B\"]\n",
    "        self.X = X\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "\n",
    "        # bの勾配\n",
    "        self.grad[\"B\"] = np.sum(dA, axis=0)\n",
    "\n",
    "        # Wの勾配       \n",
    "        self.grad[\"W\"]= np.dot(self.X.T, dA)\n",
    "    \n",
    "        # 次層へのデルタ\n",
    "        dZ = np.dot(dA, self.network[\"W\"].T)\n",
    "    \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜Scratch2dCNNClassifier＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder \n",
    "class Scratch2dCNNClassifier:\n",
    "    def __init__(self, lr, n_nodes = 14580, n_output = 10, sigma = 0.03, epoch = 1, verbose = True, Initializer = \"simple\", optimizer = \"SGD\", activation = \"Sigmoid\"):\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_output = n_output   \n",
    "        self.batch_size = 20 # バッチサイズ\n",
    "        self.epoch = epoch\n",
    "        self.train_loss_list = []\n",
    "        self.test_loss_list = []\n",
    "        self.verbose = verbose\n",
    "        self.sigma = sigma\n",
    "        self.Initializer = Initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        X = X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
    "        print(\"X.shape = \", X.shape)\n",
    "        \n",
    "        if X_val is not None:\n",
    "            X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1], X_val.shape[2])\n",
    "            print(\"X_val.shape = \", X_val.shape)\n",
    "        \n",
    "\n",
    "        # 活性化関数のインスタンス化\n",
    "        if self.activation == \"Tanh\":\n",
    "            self.activation1 = Tanh()        \n",
    "            self.activation2 = Tanh()        \n",
    "            \n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            self.activation1 = Sigmoid()        \n",
    "            self.activation2 = Sigmoid() \n",
    "            \n",
    "        elif self.activation == \"Relu\":\n",
    "            self.activation1 = Relu()        \n",
    "            self.activation2 = Relu()             \n",
    "            \n",
    "            \n",
    "        self.activation3 = Softmax()        \n",
    "        \n",
    "        # 最適化手法のインスタンス化\n",
    "        if self.optimizer == \"SGD\":\n",
    "            optimizer1 = SGD(self.lr)   \n",
    "            optimizer2 = SGD(self.lr) \n",
    "            optimizer3 = SGD(self.lr) \n",
    "        elif self.optimizer == \"AdaGrad\":\n",
    "            optimizer1 = AdaGrad(self.lr)   \n",
    "            optimizer2 = AdaGrad(self.lr)  \n",
    "            optimizer3 = AdaGrad(self.lr)  \n",
    "        elif self.optimizer == \"Adam\":\n",
    "            optimizer1 = Adam(self.lr)   \n",
    "            optimizer2 = Adam(self.lr)  \n",
    "            optimizer3 = Adam(self.lr)  \n",
    "        \n",
    "        # 全結合層のインスタンス化\n",
    "        if self.Initializer == \"simple\":\n",
    "            self.FC3 = FC(self.n_nodes, self.n_output, SimpleInitializer(self.sigma), optimizer3)\n",
    "        \n",
    "        elif self.Initializer == \"Xavier\":\n",
    "            self.FC3 = FC(self.n_nodes, self.n_output, XavierInitializer(self.sigma), optimizer3)\n",
    "        \n",
    "        elif self.Initializer == \"He\":\n",
    "            self.FC3 = FC(self.n_nodes, self.n_output, HeInitializer(self.sigma), optimizer3)\n",
    "        \n",
    "        \n",
    "        # Conv層のインスタンス化\n",
    "        self.Conv1 = Conv2d(self.batch_size, X.shape[1], 3, optimizer1, pad =1)   \n",
    "\n",
    "        # Pooling層のインスタンス化\n",
    "        self.Pool1 = MaxPool2D(pool_h=2, pool_w=2, stride=1)\n",
    "\n",
    "        # Flattern層のインスタンス化\n",
    "        self.Flattern =Flattern()\n",
    "        \n",
    "        print(\"Learning Start!\")\n",
    "        \n",
    "        # one-hot-vectol化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        if y_val is not None:\n",
    "            y_test_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "        \n",
    "        # 学習回数のカウンタ\n",
    "        learning_cnt = 0\n",
    "        \n",
    "        loss_func = loss_function()\n",
    "\n",
    "        for i in range(self.epoch):\n",
    "\n",
    "            # ミニバッチ取得\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size = self.batch_size)\n",
    "            \n",
    "            # すべてのミニバッチを抜ける前に直前の値をバックアップしておく(グラフ用に)\n",
    "            train_loss_batch = []\n",
    "            test_loss_batch = []\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # このfor文内でミニバッチが使える\n",
    "                \n",
    "                z1, z2, z3 = self._forward(mini_X_train)                \n",
    "                \n",
    "                grad = self._backward(z3, mini_y_train)\n",
    "                \n",
    "                learning_cnt += 1\n",
    "                \n",
    "                # ミニバッチ内のロスを格納\n",
    "                train_loss_batch.append(loss_func.cross_entropy_error(mini_y_train, z3))\n",
    "                 \n",
    "                #break\n",
    "\n",
    "            # loss計算\n",
    "            # ミニバッチ内のロスの平均を取る\n",
    "            train_loss_mean = np.array(train_loss_batch).mean()\n",
    "            self.train_loss_list.append(train_loss_mean)\n",
    "\n",
    "            # test_loss の初期化\n",
    "            test_loss = 0\n",
    "            if X_val is not None:\n",
    "                z1, z2, y_test_pred = self._forward(X_val)\n",
    "                test_loss = loss_func.cross_entropy_error(y_test_one_hot, y_test_pred)\n",
    "                self.test_loss_list.append(test_loss)\n",
    "\n",
    "            # test_loss差分表示処理\n",
    "            if len(self.test_loss_list) == 1:\n",
    "                diff = 0\n",
    "            else:\n",
    "                diff = -1 * (self.test_loss_list[-2] - self.test_loss_list[-1])\n",
    "            \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "                print(\"Epoch Count = {}, train_loss = {:.5f}, test_loss = {:.5f}, diff = {}\".format(i+1, train_loss_mean, test_loss, diff))\n",
    "                \n",
    "        print(\"Learning Finish!\")\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        X = X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
    "        print(\"X.shape = \", X.shape)\n",
    "        \n",
    "        z1, z2, y_test_pred = self._forward(X)\n",
    "\n",
    "        y_pred = np.argmax(y_test_pred, axis = 1)\n",
    "        return y_pred\n",
    "    \n",
    "        \n",
    "        \n",
    "    # 順伝播\n",
    "    def _forward(self, X):\n",
    "        A1 = self.Conv1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.Pool1.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        Z2 = self.Flattern.forward(Z2)    # Z2をそのまま平滑化\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "\n",
    "        return Z1, Z2, Z3\n",
    "        \n",
    "    # 誤差逆伝播\n",
    "    def _backward(self, Z3, Y):\n",
    "        dA3 = self.activation3.backward(Y, Z3) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dZ2 = self.Flattern.backward(dZ2)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.Pool1.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dZ0 = self.Conv1.backward(dA1) # dZ0は使用しない\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST読込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 処理に時間のかかる場合はデータを削減 \n",
    "X_train, y_train = X_train[:5000], y_train[:5000]\n",
    "X_test, y_test = X_test[:1000], y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train /255\n",
    "X_test = X_test /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】学習と推定\n",
    "作成したConv2dを使用してMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "精度は低くともまずは動くことを目指してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scratch2dCNN = Scratch2dCNNClassifier(lr = 0.0008, epoch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (5000, 1, 28, 28)\n",
      "X_val.shape =  (1000, 1, 28, 28)\n",
      "Learning Start!\n",
      "Epoch Count = 1, train_loss = 2.34713, test_loss = 2.29897, diff = 0\n",
      "Epoch Count = 2, train_loss = 2.27834, test_loss = 2.26189, diff = -0.03707522084929815\n",
      "Epoch Count = 3, train_loss = 2.23922, test_loss = 2.22569, diff = -0.036202844202280815\n",
      "Epoch Count = 4, train_loss = 2.20103, test_loss = 2.19035, diff = -0.03533666036778449\n",
      "Epoch Count = 5, train_loss = 2.16376, test_loss = 2.15588, diff = -0.03447775086948823\n",
      "Epoch Count = 6, train_loss = 2.12740, test_loss = 2.12225, diff = -0.03362720283946041\n",
      "Epoch Count = 7, train_loss = 2.09195, test_loss = 2.08946, diff = -0.032786094147462475\n",
      "Epoch Count = 8, train_loss = 2.05739, test_loss = 2.05751, diff = -0.03195547828862777\n",
      "Epoch Count = 9, train_loss = 2.02372, test_loss = 2.02637, diff = -0.031136369666172392\n",
      "Epoch Count = 10, train_loss = 1.99092, test_loss = 1.99604, diff = -0.030329729872256683\n",
      "Epoch Count = 11, train_loss = 1.95897, test_loss = 1.96650, diff = -0.029536455489072466\n",
      "Epoch Count = 12, train_loss = 1.92787, test_loss = 1.93775, diff = -0.028757367815342993\n",
      "Epoch Count = 13, train_loss = 1.89759, test_loss = 1.90975, diff = -0.02799320478641487\n",
      "Epoch Count = 14, train_loss = 1.86812, test_loss = 1.88251, diff = -0.02724461521426247\n",
      "Epoch Count = 15, train_loss = 1.83945, test_loss = 1.85600, diff = -0.02651215534117557\n",
      "Epoch Count = 16, train_loss = 1.81155, test_loss = 1.83020, diff = -0.025796287587743327\n",
      "Epoch Count = 17, train_loss = 1.78440, test_loss = 1.80510, diff = -0.025097381288456067\n",
      "Epoch Count = 18, train_loss = 1.75800, test_loss = 1.78069, diff = -0.024415715148719874\n",
      "Epoch Count = 19, train_loss = 1.73231, test_loss = 1.75694, diff = -0.023751481124614227\n",
      "Epoch Count = 20, train_loss = 1.70733, test_loss = 1.73383, diff = -0.02310478941751981\n",
      "Epoch Count = 21, train_loss = 1.68303, test_loss = 1.71136, diff = -0.022475674285456604\n",
      "Epoch Count = 22, train_loss = 1.65940, test_loss = 1.68949, diff = -0.02186410039618214\n",
      "Epoch Count = 23, train_loss = 1.63641, test_loss = 1.66822, diff = -0.02126996947913118\n",
      "Epoch Count = 24, train_loss = 1.61405, test_loss = 1.64753, diff = -0.02069312706977655\n",
      "Epoch Count = 25, train_loss = 1.59229, test_loss = 1.62740, diff = -0.020133369177610883\n",
      "Epoch Count = 26, train_loss = 1.57113, test_loss = 1.60781, diff = -0.019590448745297584\n",
      "Epoch Count = 27, train_loss = 1.55055, test_loss = 1.58874, diff = -0.01906408179977981\n",
      "Epoch Count = 28, train_loss = 1.53052, test_loss = 1.57019, diff = -0.01855395322554587\n",
      "Epoch Count = 29, train_loss = 1.51103, test_loss = 1.55213, diff = -0.01805972211521678\n",
      "Epoch Count = 30, train_loss = 1.49206, test_loss = 1.53455, diff = -0.01758102667323924\n",
      "Epoch Count = 31, train_loss = 1.47360, test_loss = 1.51743, diff = -0.017117488664940117\n",
      "Epoch Count = 32, train_loss = 1.45563, test_loss = 1.50076, diff = -0.016668717415944334\n",
      "Epoch Count = 33, train_loss = 1.43814, test_loss = 1.48453, diff = -0.016234313376438658\n",
      "Epoch Count = 34, train_loss = 1.42110, test_loss = 1.46871, diff = -0.015813871271506352\n",
      "Epoch Count = 35, train_loss = 1.40451, test_loss = 1.45331, diff = -0.015406982863254814\n",
      "Epoch Count = 36, train_loss = 1.38836, test_loss = 1.43829, diff = -0.015013239353101948\n",
      "Epoch Count = 37, train_loss = 1.37262, test_loss = 1.42366, diff = -0.014632233453861332\n",
      "Epoch Count = 38, train_loss = 1.35728, test_loss = 1.40940, diff = -0.014263561161436122\n",
      "Epoch Count = 39, train_loss = 1.34234, test_loss = 1.39549, diff = -0.013906823255306788\n",
      "Epoch Count = 40, train_loss = 1.32777, test_loss = 1.38193, diff = -0.013561626555832929\n",
      "Epoch Count = 41, train_loss = 1.31357, test_loss = 1.36870, diff = -0.01322758496482046\n",
      "Epoch Count = 42, train_loss = 1.29973, test_loss = 1.35580, diff = -0.012904320314037543\n",
      "Epoch Count = 43, train_loss = 1.28623, test_loss = 1.34320, diff = -0.012591463044438811\n",
      "Epoch Count = 44, train_loss = 1.27306, test_loss = 1.33092, diff = -0.012288652736947903\n",
      "Epoch Count = 45, train_loss = 1.26021, test_loss = 1.31892, diff = -0.011995538513698278\n",
      "Epoch Count = 46, train_loss = 1.24768, test_loss = 1.30721, diff = -0.011711779326822214\n",
      "Epoch Count = 47, train_loss = 1.23545, test_loss = 1.29577, diff = -0.011437044150099496\n",
      "Epoch Count = 48, train_loss = 1.22351, test_loss = 1.28460, diff = -0.011171012087166643\n",
      "Epoch Count = 49, train_loss = 1.21185, test_loss = 1.27369, diff = -0.010913372408434707\n",
      "Epoch Count = 50, train_loss = 1.20046, test_loss = 1.26302, diff = -0.010663824527527899\n",
      "Learning Finish!\n"
     ]
    }
   ],
   "source": [
    "Scratch2dCNN.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 28, 28)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (1000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "y_pred = Scratch2dCNN.predict(X_test)\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.936709</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.814433</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.796668</td>\n",
       "      <td>0.790706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.091954</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.684596</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.891429</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>0.627219</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.806122</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.643145</td>\n",
       "      <td>0.653851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0           1           2           3           4  \\\n",
       "precision   0.866667    0.729412    0.936709    0.756757    0.464912   \n",
       "recall      0.917647    0.984127    0.637931    0.785047    0.963636   \n",
       "f1-score    0.891429    0.837838    0.758974    0.770642    0.627219   \n",
       "support    85.000000  126.000000  116.000000  107.000000  110.000000   \n",
       "\n",
       "                   5          6          7          8          9  accuracy  \\\n",
       "precision   1.000000   0.835294   0.814433   0.562500   1.000000       0.7   \n",
       "recall      0.091954   0.816092   0.797980   0.808989   0.042553       0.7   \n",
       "f1-score    0.168421   0.825581   0.806122   0.663594   0.081633       0.7   \n",
       "support    87.000000  87.000000  99.000000  89.000000  94.000000       0.7   \n",
       "\n",
       "             macro avg  weighted avg  \n",
       "precision     0.796668      0.790706  \n",
       "recall        0.684596      0.700000  \n",
       "f1-score      0.643145      0.653851  \n",
       "support    1000.000000   1000.000000  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# pandas.DataFrameへ変換\n",
    "df_cr = pd.DataFrame(cr)\n",
    "df_cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACC = 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d4b3de3a48>]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(len(Scratch2dCNN.train_loss_list)), Scratch2dCNN.train_loss_list, color = \"blue\")\n",
    "plt.plot(range(len(Scratch2dCNN.test_loss_list)), Scratch2dCNN.test_loss_list, color = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まだまだ下がりそうではあるが、相当な時間がかかる為、省略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip【問題7】（アドバンス課題）LeNet\n",
    "CNNで画像認識を行う際は、フィルタサイズや層の数などを１から考えるのではなく、有名な構造を利用することが一般的です。現在では実用的に使われることはありませんが、歴史的に重要なのは1998年の LeNet です。この構造を再現してMNISTに対して動かし、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n",
    "\n",
    "<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/a/arakan_no_boku/20170923/20170923102525.jpg\">\n",
    "\n",
    "※上記論文から引用\n",
    "\n",
    "\n",
    "サブサンプリングとは現在のプーリングに相当するものです。現代風に以下のように作ってみることにします。活性化関数も当時はシグモイド関数ですが、ReLUとします。\n",
    "\n",
    "\n",
    "畳み込み層　出力チャンネル数6、フィルタサイズ5×5、ストライド1  \n",
    "ReLU  \n",
    "最大プーリング  \n",
    "畳み込み層　出力チャンネル数16、フィルタサイズ5×5、ストライド1  \n",
    "ReLU  \n",
    "最大プーリング  \n",
    "平滑化  \n",
    "全結合層　出力ノード数120  \n",
    "ReLU  \n",
    "全結合層　出力ノード数84  \n",
    "ReLU  \n",
    "全結合層　出力ノード数10  \n",
    "ソフトマックス関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】（アドバンス課題）有名な画像認識モデルの調査"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNの代表的な構造としてははAlexNet(2012)、VGG16(2014)などがあります。こういったものはフレームワークで既に用意されていることも多いです。\n",
    "\n",
    "\n",
    "どういったものがあるか簡単に調べてまとめてください。名前だけでも見ておくと良いでしょう。\n",
    "\n",
    "\n",
    "《参考》\n",
    "\n",
    "Applications - Keras Documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＜LeNet＞\n",
    "1998年に提案された、現Facebook AI ResearchのYann LeCun先生によるCNNの元祖となるネットワーク。畳込み層とプーリング層を交互に重ねたネットワークで、この時点ですでに現在使われているアーキテクチャとほぼ同じ形になっている。活性化関数がシグモイド関数な点、プーリング層がMaxプーリングではなくサブサンプリングで縮小している点などが特徴。\n",
    "\n",
    "<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/a/arakan_no_boku/20170923/20170923102525.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＜AlexNet＞\n",
    "2012年のImageNetを使った画像認識コンペILSVRCで圧倒的な成績を残し、ディープラーニングの火付け役となった。\n",
    "\n",
    "畳み込み層とプーリング層、そして正規化層と呼ばれる局所的正規化(LRN ― Local Response Normalization)を行う層を重ねた14層のネットワーク。活性化関数はReLUで、Dropoutを用いているのが特徴。他にもData Augmentationなども行っていて、現在使われているCNNでの主要な手法がこの時点ですでに活用されている。\n",
    "\n",
    "<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F209049%2Fbbf90731-67c0-2d75-f64e-1272ddebf653.jpeg?ixlib=rb-1.2.2&auto=format&gif-q=60&q=75&w=1400&fit=max&s=ac4d8631c8b11fdd1825827b2c7c6947\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＜GoogLeNet＞(Inception-v3)\n",
    "2014年のコンペで1位になったアーキテクチャ。このアーキテクチャは通常の入力層から出力層まで縦一直線な構造ではなく、インセプション構造と呼ばれる横にも層が広がる構造をしている。このため、Inceptionモデルとも呼ばれる。\n",
    "\n",
    "\n",
    "<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F67f98213-a8ff-de52-2d7e-18a3ef37cdd2.png?ixlib=rb-1.2.2&auto=format&gif-q=60&q=75&s=ad62bf7055736fceb31f9d616d9fcb56\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＜VGG＞\n",
    "2014年のILSVRCで2位になった、オックスフォード大学のVGGチームのネットワーク。AlexNetをより深くした、畳み込み層とプーリング層から成るどノーマルなCNNで、重みがある層(畳み込み層や全結合層)を16層、もしくは19層重ねたもの。それぞれVGG16やVGG19と呼ばれる。\n",
    "\n",
    "<img src=\"https://img2018.cnblogs.com/blog/583030/201909/583030-20190912164331595-1580838215.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[rain]:http://2.bp.blogspot.com/-GkAkf2Df5JY/VgJ6ytq6XAI/AAAAAAAAOWE/Vx5l-MeObbo/s1600/rain.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＜ResNet＞\n",
    "Microsoft(当時)のKaiming He氏による、2015年のILSVRCで優勝したネットワーク。それまでのネットワークでは層を深くしすぎると性能が落ちるという問題があったが、それを「スキップ構造」によって解決し、152層もの深さ(前年優勝のGoogLeNetでも22層)を実現した。\n",
    "\n",
    "スキップ構造は、ある層への入力をバイパスし層をまたいで奥の層へ入力してしまうというもので、これにより勾配の消失や発散を防止し、超多層のネットワークを実現している。\n",
    "\n",
    "<img src=\"https://www.bigdata-navi.com/aidrops/wp-content/uploads/2020/03/1-198x1200.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題9】出力サイズとパラメータ数の計算\n",
    "\n",
    "CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    "\n",
    "\n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。\n",
    "\n",
    "\n",
    "以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "１.  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
    "\n",
    "入力サイズ : 144×144, 3チャンネル  \n",
    "フィルタサイズ : 3×3, 6チャンネル  \n",
    "ストライド : 1  \n",
    "パディング : なし  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_{h,out} =  \\frac{N_{h,in}+2P_{h}-F_{h}}{S_{h}} + 1 = \\frac{144+2*0-3}{1} + 1 = 142\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_{w,out} =  \\frac{N_{w,in}+2P_{w}-F_{w}}{S_{w}} + 1 = \\frac{144+2*0-3}{1} + 1 = 142\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上から、出力として、142×142, 6ch　の出力マップが得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ数は、  \n",
    "フィルターの縦 * フィルターの横 * 入力のチャンネル数 * 出力のチャンネル数 + バイアス(1チャンネルごとに1つ)　から  \n",
    "3(FH) * 3(FW) * 3(in_ch) * 6(out_ch) + 6(B) = 168コ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "２.  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
    "\n",
    "入力サイズ : 60×60, 24チャンネル  \n",
    "フィルタサイズ : 3×3, 48チャンネル  \n",
    "ストライド　: 1  \n",
    "パディング : なし  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_{h,out} =  \\frac{N_{h,in}+2P_{h}-F_{h}}{S_{h}} + 1 = \\frac{60+2*0-3}{1} + 1 = 58\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "N_{w,out} =  \\frac{N_{w,in}+2P_{w}-F_{w}}{S_{w}} + 1 = \\frac{60+2*0-3}{1} + 1 = 58\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上から、出力として、58×58, 48ch　の出力マップが得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ数は、  \n",
    "フィルターの縦 * フィルターの横 * 入力のチャンネル数 * 出力のチャンネル数 + バイアス(1チャンネルごとに1つ)　から  \n",
    "3(FH) * 3(FW) * 24(in_ch) * 48(out_ch) + 48(B) = 10416コ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "３.  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
    "\n",
    "入力サイズ : 20×20, 10チャンネル  \n",
    "フィルタサイズ: 3×3, 20チャンネル  \n",
    "ストライド : 2  \n",
    "パディング : なし  \n",
    "\n",
    "\n",
    "＊最後の例は丁度良く畳み込みをすることができない場合です。フレームワークでは余ったピクセルを見ないという処理が行われることがあるので、  \n",
    "その場合を考えて計算してください。端が欠けてしまうので、こういった設定は好ましくないという例です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_{h,out} =  \\frac{N_{h,in}+2P_{h}-F_{h}}{S_{h}} + 1 = \\frac{20+2*0-3}{2} + 1 ≒ 9　(切り捨て)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "N_{w,out} =  \\frac{N_{w,in}+2P_{w}-F_{w}}{S_{w}} + 1 = \\frac{20+2*0-3}{2} + 1 ≒ 9　(切り捨て)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上から、出力として、9×9, 20ch　の出力マップが得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ数は、  \n",
    "フィルターの縦 * フィルターの横 * 入力のチャンネル数 * 出力のチャンネル数 + バイアス(1チャンネルごとに1つ)　から  \n",
    "3(FH) * 3(FW) * 10(in_ch) * 20(out_ch) + 20(B) = 1820コ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題10】（アドバンス課題）フィルタサイズに関する調査\n",
    "畳み込み層にはフィルタサイズというハイパーパラメータがありますが、2次元畳み込み層において現在では3×3と1×1の使用が大半です。以下のそれぞれを調べたり、自分なりに考えて説明してください。\n",
    "\n",
    "\n",
    "7×7などの大きめのものではなく、3×3のフィルタが一般的に使われる理由\n",
    "高さや幅方向を持たない1×1のフィルタの効果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "①CNNでは、MaxPoolingが主に使われるため、フィルターサイズを7×7などの大きな値にすると、大域最大値で丸められるため、局所的な特徴が捨てられてしまう。  \n",
    "\n",
    "②例えば、(縦, 横, チャンネル)=(100, 100, 32)の層を、3×3の畳み込み層を用いて(100,100,64)にすることを考えます。\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*YBaxCyPtG8knAGWWNcaiVg.png\">\n",
    "\n",
    "この時、上図のようにストレートに畳み込み層を実装すると、必要な変数は3×3x32×64+64=18496個です。では次に、この2つの層の間に1×1の畳み込み層を挟んでみましょう。この時、1×1畳み込み層の出力チャンネルは32より小さくなるようにします。今回は32の半分の16とします。\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*Dvly7btqkdj9jun3IK--ow.png\">\n",
    "\n",
    "すると、必要な変数は\n",
    "・最初の畳み込み層:1×1x32×16+16=528\n",
    "・次の畳み込み層:3×3x16×64+64=9280\n",
    "・計：9808\n",
    "\n",
    "となり、層の数は増えているにもかかわらず、元の構造からパラメータの数を半分近くまで落とすことに成功しました。このように、1×1の畳み込み層を挟むことでモデル全体のサイズを小さくできるため、計算能力が低い環境や、低電力の環境でもモデルを動かすことが可能となります。\n",
    "\n",
    "<a href=\"https://medium.com/lsc-psd/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%81%AA%E3%82%89%E7%AD%94%E3%81%88%E3%82%89%E3%82%8C%E3%81%A6%E5%BD%93%E7%84%B6%E3%81%AE4%E3%81%A4%E3%81%AE%E5%95%8F%E9%A1%8C-%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E5%B1%A4%E7%B7%A8-659809195d2\" target=\"_blank\">機械学習エンジニアなら答えられて当然の問題4問 ～畳み込み層編～  問題３</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
