{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint22 深層学習スクラッチ リカレントニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "### Sprintの目的\n",
    "スクラッチを通してリカレントニューラルネットワークの基礎を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "スクラッチでリカレントニューラルネットワークの実装を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.リカレントニューラルネットワークスクラッチ\n",
    "リカレントニューラルネットワーク（RNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。\n",
    "\n",
    "\n",
    "クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
    "\n",
    "\n",
    "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "$$\n",
    "a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + B\\\\\n",
    "h_t = tanh(a_t)\n",
    "$$\n",
    "$a_t$ : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "$h_t$ : 時刻tの状態・出力 (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "$x_{t}$ : 時刻tの入力 (batch_size, n_features)\n",
    "\n",
    "\n",
    "$W_{x}$ : 入力に対する重み (n_features, n_nodes)\n",
    "\n",
    "\n",
    "$h_{t-1}$ : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "$W_{h}$ : 状態に対する重み。 (n_nodes, n_nodes)\n",
    "\n",
    "\n",
    "$B$ : バイアス項 (n_nodes,)\n",
    "\n",
    "\n",
    "初期状態 h_{0} は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
    "\n",
    "\n",
    "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 x は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
    "\n",
    "\n",
    "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＜参考＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        self.network = {}\n",
    "        self.network[\"W\"] = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.network[\"B\"] = initializer.B(n_nodes2)\n",
    "        self.A = 0\n",
    "        self.grad = {}\n",
    "        self.X = 0\n",
    "\n",
    "        pass\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        # 自層への入力\n",
    "        A =  np.dot(X, self.network[\"W\"] ) + self.network[\"B\"]\n",
    "        self.X = X\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "\n",
    "        # bの勾配\n",
    "        self.grad[\"B\"] = np.sum(dA, axis=0)\n",
    "\n",
    "        # Wの勾配       \n",
    "        self.grad[\"W\"]= np.dot(self.X.T, dA)\n",
    "    \n",
    "        # 次層へのデルタ\n",
    "        dZ = np.dot(dA, self.network[\"W\"].T)\n",
    "    \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, n_sequences, n_features, n_nodes, initializer = None, \n",
    "                 optimizer = None, w_x = None, w_h = None, b = None, h = None):\n",
    "\n",
    "        # 初期化\n",
    "        self.network = {}\n",
    "        \n",
    "        if w_x is None:\n",
    "            self.network[\"w_x\"] = initializer.W(n_features, n_nodes)\n",
    "        else:\n",
    "            self.network[\"w_x\"] = w_x\n",
    "            \n",
    "        if w_h is None:\n",
    "            self.network[\"w_h\"] = initializer.W(n_nodes, n_nodes)\n",
    "        else:\n",
    "            self.network[\"w_h\"] = w_h\n",
    "            \n",
    "        if h is None:\n",
    "            self.network[\"h\"] = np.zeros((batch_size, n_nodes))\n",
    "        else:\n",
    "            self.network[\"h\"] = h\n",
    "            \n",
    "        if b is None:\n",
    "            self.network[\"B\"] = initializer.B(n_nodes)\n",
    "        else:\n",
    "            self.network[\"B\"] = b\n",
    "            \n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer        \n",
    "            \n",
    "        self.sequences = n_sequences\n",
    "            \n",
    "        self.At = 0\n",
    "        self.grad = {}\n",
    "        self.X = 0\n",
    "        \n",
    "    # 順伝播\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        # 自層への入力\n",
    "        for t in range(self.sequences):\n",
    "            self.At =  np.dot(X[:,t], self.network[\"w_x\"] ) + np.dot(self.network[\"h\"], self.network[\"w_h\"]) + self.network[\"B\"]\n",
    "            self.network[\"h\"] = np.tanh(self.At)\n",
    "            #print(\"self.At.shape\", self.At.shape)\n",
    "            #print(\"h.shape\", self.network[\"h\"].shape)\n",
    "        \n",
    "        self.X = X\n",
    "\n",
    "        #print(\"h = \", self.network[\"h\"] )\n",
    "        return self.network[\"h\"] \n",
    "\n",
    "    # 誤差逆伝播\n",
    "    def backward(self, dh):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "\n",
    "        # hの勾配\n",
    "        self.grad[\"h\"] = dh * (1 - self.network[\"h\"]**2)\n",
    "        \n",
    "        # bの勾配\n",
    "        self.grad[\"B\"] = np.sum(self.grad[\"h\"] , axis=0)\n",
    "\n",
    "        # w_xの勾配       \n",
    "        self.grad[\"w_x\"] = np.dot(self.networl[\"w_x\"].T, dA)\n",
    "        \n",
    "        # w_hの勾配       \n",
    "        self.grad[\"w_h\"] = np.dot(elf.networl[\"w_h\"].T, dA)        \n",
    "    \n",
    "        # 次層へのデルタ\n",
    "        dZ = np.dot(dA, self.network[\"W\"].T)\n",
    "    \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # SGD =====================================================\n",
    "        for key in ('W','B'):\n",
    "            layer.network[key] -= self.lr * layer.grad[key] \n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。\n",
    "```python\n",
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
    "```\n",
    "フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。\n",
    "\n",
    "```python\n",
    "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def __init__(sel, batch_size, n_sequences, n_features, n_nodes, initializer, optimizer, w_x = None, w_h = None, b = None, h = None):\n",
    "sRNN = SimpleRNN(batch_size = batch_size, \n",
    "                             n_sequences = n_sequences, \n",
    "                             n_features = n_features, \n",
    "                             n_nodes = n_nodes,\n",
    "                             w_x = w_x,\n",
    "                             w_h = w_h,\n",
    "                             b = b,\n",
    "                             h = h\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = sRNN.forward(x)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】（アドバンス課題）バックプロパゲーションの実装\n",
    "バックプロパゲーションを実装してください。\n",
    "\n",
    "\n",
    "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。\n",
    "\n",
    "$$\n",
    "W_x^{\\prime} = W_x - \\alpha \\frac{\\partial L}{\\partial W_x} \\\\\n",
    "W_h^{\\prime} = W_h - \\alpha \\frac{\\partial L}{\\partial W_h} \\\\\n",
    "B^{\\prime} = B - \\alpha \\frac{\\partial L}{\\partial B}\n",
    "$$\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x}$ : $W_x$ に関する損失 L の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h}$ : $W_h$ に関する損失 L の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B}$ : $B$ に関する損失 L の勾配\n",
    "\n",
    "\n",
    "勾配を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} ×(1-tanh^2(a_t))\n",
    "\\frac{\\partial L}{\\partial B} = \\frac{\\partial h_t}{\\partial a_t}\n",
    "\\frac{\\partial L}{\\partial W_x} = x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
    "\\frac{\\partial L}{\\partial W_h} = h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "＊$\\frac{\\partial L}{\\partial h_t}$ は前の時刻からの状態の誤差と出力の誤差の合計です。hは順伝播時に出力と次の層に伝わる状態双方に使われているからです。\n",
    "\n",
    "\n",
    "前の時刻や層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}\n",
    "\\frac{\\partial L}{\\partial x_{t}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
