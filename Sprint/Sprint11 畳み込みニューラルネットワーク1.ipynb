{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 深層学習スクラッチ 畳み込みニューラルネットワーク1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.このSprintについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprintの目的\n",
    "スクラッチを通してCNNの基礎を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "スクラッチで1次元用畳み込みニューラルネットワークを実装した後、学習と検証を行なっていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1次元の畳み込みニューラルネットワークスクラッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みニューラルネットワーク（CNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
    "\n",
    "\n",
    "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1次元畳み込み層とは\n",
    "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
    "\n",
    "\n",
    "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの用意\n",
    "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "\n",
    "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "$a_i$ : 出力される配列のi番目の値\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$x_{(i+s)}$ : 入力の配列の(i+s)番目の値\n",
    "\n",
    "\n",
    "$w_s$ : 重みの配列のs番目の値\n",
    "\n",
    "\n",
    "$b$ : バイアス項\n",
    "\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "\n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
    "\n",
    "$$\n",
    "w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n",
    "b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$ : $w_s$ に関する損失 L の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 L の勾配\n",
    "\n",
    "\n",
    "勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi番目の値\n",
    "\n",
    "\n",
    "$N_{out}$ : 出力のサイズ\n",
    "\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "\n",
    "ただし、 j-s<0 または $j-s>N_{out}-1$ のとき $\\frac{\\partial L}{\\partial a_{(j-s)}} =0$ です。\n",
    "\n",
    "\n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜SimpleConv1d＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, F_cnt, F_ch, F_size, optimizer, stride = 1, pad = 0):\n",
    "        # 初期化\n",
    "        self.network = {}\n",
    "        self.network[\"W\"] = np.random.randn(F_cnt, F_ch, F_size, F_size)\n",
    "        self.network[\"B\"] = np.random.randn(F_cnt)\n",
    "\n",
    "        self.F_cnt = F_cnt\n",
    "        self.F_ch = F_ch\n",
    "        self.F_size = F_size\n",
    "        \n",
    "        self.optimizer = optimizer        \n",
    "        \n",
    "        self.grad = {}\n",
    "        self.X = 0\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, X, W = None, B = None):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"  \n",
    "        \n",
    "        '''\n",
    "        if self.A is None:\n",
    "            self.A = np.random.randn(X.shape[0] - self.filter_size + 1)\n",
    "        '''\n",
    "        if W is not None:\n",
    "            self.network[\"W\"] = W\n",
    "            \n",
    "        if B is not None:\n",
    "            self.network[\"B\"] = B\n",
    "            \n",
    "        self.A = np.zeros(X.shape[0] - self.network[\"W\"].shape[0] + 1)\n",
    "        \n",
    "        for i in range(X.shape[0] - self.network[\"W\"].shape[0] + 1):\n",
    "            A = np.sum(np.dot(X[i:i + self.F_size], self.network[\"W\"] )) + self.network[\"B\"]\n",
    "            self.X = X\n",
    "            self.A[i] = A\n",
    "\n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 前層でのデルタ\n",
    "\n",
    "        self.grad = {}\n",
    "        print(\"len(self.X) = \", len(self.X))\n",
    "        print(\"self.F_size = \", self.F_size)\n",
    "        self.grad[\"W\"] = np.zeros(len(self.network[\"W\"]))\n",
    "        self.grad[\"B\"] = np.zeros(len(self.network[\"B\"]))\n",
    "        dx = np.zeros(len(self.X))\n",
    "        \n",
    "        \n",
    "        # bの勾配\n",
    "        self.grad[\"B\"] = np.sum(dA)    \n",
    "        print(\"self.grad[B][j] = \", self.grad[\"B\"])        \n",
    "        \n",
    "        for j in range(len(self.X) - len(dA) + 1):\n",
    "            print(\"j = \", j)\n",
    "            # Wの勾配       \n",
    "            self.grad[\"W\"][j] += np.sum(self.X[j:j + len(dA)] * dA)\n",
    "            print(\"self.grad[W][j] = \" ,self.grad[\"W\"][j])\n",
    "\n",
    "            # 2層でのデルタ\n",
    "            dx[j:j + len(dA)] += dA * self.network[\"W\"][j]\n",
    "            print(\"dx[j] = \", dx[j])\n",
    "    \n",
    "        ## 更新\n",
    "        #self = self.optimizer.update(self)\n",
    "        \n",
    "        return self.grad[\"B\"], self.grad[\"W\"], dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ＜SGD＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        #print(\"★★★SGD: Update\")\n",
    "        # SGD =====================================================\n",
    "        for key in ('W','B'):\n",
    "            layer.network[key] -= self.lr * layer.grad[key] \n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ＜SimpleInitializer＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
    "\n",
    "$$\n",
    "N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\\\\\n",
    "$$\n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$S$ : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_out(input_size, padding, filter_size, straide):\n",
    "    Nout = (input_size + 2*padding - filter_size)/straide + 1\n",
    "    \n",
    "    return Nout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
    "\n",
    "\n",
    "入力x、重みw、バイアスbを次のようにします。\n",
    "\n",
    "```python\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "```\n",
    "フォワードプロパゲーションをすると出力は次のようになります。\n",
    "\n",
    "```python\n",
    "a = np.array([35, 50])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = SimpleInitializer(sigma = 0.03)\n",
    "\n",
    "simpConv1d = SimpleConv1d(F_cnt = 1, F_ch = 1, F_size = len(w), optimizer = \"SGD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35., 50.])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = simpConv1d.forward(x, w, b)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n",
    "```python\n",
    "delta_a = np.array([10, 20])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(self.X) =  4\n",
      "self.F_size =  3\n",
      "self.grad[B][j] =  30\n",
      "j =  0\n",
      "self.grad[W][j] =  50.0\n",
      "dx[j] =  30.0\n",
      "j =  1\n",
      "self.grad[W][j] =  80.0\n",
      "dx[j] =  110.0\n",
      "j =  2\n",
      "self.grad[W][j] =  110.0\n",
      "dx[j] =  170.0\n"
     ]
    }
   ],
   "source": [
    "delta_a = np.array([10, 20])\n",
    "delta_b, delta_w, delta_x = simpConv1d.backward(delta_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バックプロパゲーションをすると次のような値になります。\n",
    "\n",
    "```python\n",
    "delta_b = np.array([30])\n",
    "delta_w = np.array([50, 80, 110])\n",
    "delta_x = np.array([30, 110, 170, 140])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50.,  80., 110.])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30., 110., 170., 140.])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 実装上の工夫\n",
    "畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "バイアス項は単純な足し算のため、重みの部分を見ます。\n",
    "\n",
    "$$\n",
    "\\sum_{s=0}^{F-1}x_{(i+s)}w_s\n",
    "$$\n",
    "これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。\n",
    "\n",
    "```python\n",
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "a = np.empty((2, 3))\n",
    "indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "a = a.sum(axis=1)\n",
    "```\n",
    "\n",
    "ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
    "\n",
    "また、二次元配列を使えば一次元配列から二次元配列が取り出せます。\n",
    "\n",
    "```python\n",
    "x = np.array([1, 2, 3, 4])\n",
    "indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])\n",
    "```\n",
    "\n",
    "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
    "\n",
    "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 《参考》\n",
    "\n",
    "以下のページのInteger array indexingの部分がこの方法についての記述です。\n",
    "\n",
    "Indexing — NumPy v1.17 Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "\n",
    "\n",
    "例えば以下のようなx, w, bがあった場合は、\n",
    "\n",
    "```python\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力は次のようになります。\n",
    "\n",
    "```python\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。\n",
    "```\n",
    "\n",
    "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
    "\n",
    "#### 《補足》\n",
    "\n",
    "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
    "\n",
    "\n",
    "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像データを２次元配列に変換\n",
    "'''\n",
    "input_data: 入力値\n",
    "filter_w: フィルターの横幅\n",
    "stride: ストライド\n",
    "pad: パディング\n",
    "'''\n",
    "def im2col(input_data, filter_w, stride=1, pad=0):\n",
    "    # N: number, C: channel, H: height, W: width\n",
    "    N, C, H, W = input_data.shape\n",
    "\n",
    "    out_w = (W + 2 * pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_w, out_w))\n",
    "\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + stride * out_w\n",
    "        col[:, :, x, :] = img[:, :,  x:x_max:stride]\n",
    "                                          #  0  1              2               3         0     2               3    1              \n",
    "    col = col.transpose(0, 2, 3, 1) # (N, C, filter_h, filter_w, out_h, out_w) -> (N, filter_w, out_h, out_w, C, filter_h)    \n",
    "    \n",
    "    col = col.reshape(N * out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜Conv1d＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, F_cnt, F_ch, F_size, optimizer, stride = 1, pad = 0):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        self.network = {}\n",
    "        self.network[\"W\"] = np.random.randn(F_cnt, F_ch, F_size)\n",
    "        self.network[\"B\"] = np.random.randn(F_cnt)        \n",
    "\n",
    "        self.F_cnt = F_cnt\n",
    "        self.F_ch = F_ch\n",
    "        self.F_size = F_size\n",
    "        \n",
    "        self.grad = {}\n",
    "        self.X = 0\n",
    "\n",
    "\n",
    "    def forward(self, X, W = None, B = None):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"  \n",
    "        \n",
    "        if W is not None:\n",
    "            self.network[\"W\"] = W\n",
    "            \n",
    "        if B is not None:\n",
    "            self.network[\"B\"] = B\n",
    "            \n",
    "            \n",
    "        # X は３次元を想定　バッチサイズ × ch数 × フィルターサイズ（横幅）  \n",
    "        self.X = X   \n",
    "\n",
    "        \n",
    "        # バッチ数\n",
    "        N_bat = X.shape[0]\n",
    "        \n",
    "        # 出力フィルター数\n",
    "        F_cnt = self.F_cnt\n",
    "        \n",
    "        # ch数\n",
    "        ch_cnt = X.shape[1]\n",
    "        \n",
    "        # ストライドカウント\n",
    "        S_cnt = X.shape[2] - self.network[\"W\"].shape[2] + 1 \n",
    "        \n",
    "        OW = int(self.N_out(X.shape[2], 0, self.network[\"W\"].shape[2], straide = 1 ))\n",
    "        \n",
    "        self.A = np.zeros((N_bat, F_cnt, OW))\n",
    "\n",
    "        \n",
    "        # バッチサイズ分ループ\n",
    "        for n in range(N_bat):\n",
    "\n",
    "            # 出力フィルター分ループ\n",
    "            for F in range(F_cnt):\n",
    "\n",
    "                # チャンネル方向へループ\n",
    "                for ch in range(ch_cnt):\n",
    "\n",
    "                    # 横方向へストライド\n",
    "                    for s in range(S_cnt):\n",
    "                                                \n",
    "                        A = np.sum( np.dot( X[n, ch, s:s + self.network[\"W\"].shape[2] ], self.network[\"W\"][F][ch].T ))\n",
    "\n",
    "                        A =A.reshape(1, -1)\n",
    "\n",
    "                        self.A[n, F, s] += A\n",
    "\n",
    "\n",
    "                \n",
    "        # バイアス項を加算\n",
    "        self.A += self.network[\"B\"].reshape(1,-1,1)\n",
    "        return self.A\n",
    "        \n",
    "    \n",
    "    \n",
    "    # ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
    "    # ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        if dA.ndim < 3:\n",
    "            # dAのバッチサイズ用の次元を増やす\n",
    "            dA = dA[np.newaxis, :, :]\n",
    "            \n",
    "\n",
    "        self.grad = {}\n",
    "        self.grad[\"W\"] = np.zeros(self.network[\"W\"].shape)\n",
    "        self.grad[\"B\"] = np.zeros(self.network[\"B\"].shape)\n",
    "        dx = np.zeros((self.X.shape))\n",
    "        \n",
    "        # バッチ数\n",
    "        N_bat = self.X.shape[0]    \n",
    "        \n",
    "        # 出力フィルター数\n",
    "        F_cnt = self.grad[\"W\"].shape[0]\n",
    "        \n",
    "        # 入力チャンネル数\n",
    "        ch_cnt =  self.X.shape[1]\n",
    "        \n",
    "        # ストライド数\n",
    "        S_cnt = self.X.shape[2] - dA.shape[2] + 1\n",
    "        \n",
    "        # bの勾配\n",
    "        self.grad[\"B\"] = np.sum(dA, axis=(1,2))       \n",
    "        \n",
    "        # バッチ方向にループ\n",
    "        for n in range(N_bat):\n",
    "            \n",
    "            # 出力フィルター方向にループ\n",
    "            for F in range(F_cnt):\n",
    "                \n",
    "                # 入力ch方向にループ\n",
    "                for ch in range(ch_cnt):\n",
    "                    \n",
    "                    # ストライド方向にループ\n",
    "                    for s in range(S_cnt):\n",
    "                        # Wの勾配       \n",
    "                        test1 = self.X[n, ch, s:s + dA.shape[2]]\n",
    "                        test2 = dA[n][F]\n",
    "                        self.grad[\"W\"][F][ch][s] += np.sum(self.X[n, ch, s:s + dA.shape[2]] * dA[n][F])\n",
    "\n",
    "                        # 2層でのデルタ\n",
    "                        test = dA[n][F] * self.network[\"W\"][F][ch][s]\n",
    "\n",
    "                        dx[n, ch, s:s + dA.shape[2]] += dA[n][F] * self.network[\"W\"][F][ch][s]\n",
    "\n",
    "\n",
    "        ## 更新\n",
    "        #self = self.optimizer.update(self)\n",
    "\n",
    "        return dx\n",
    "\n",
    "    \n",
    "    def N_out(self, input_size, padding, filter_size, straide):\n",
    "        Nout = (input_size + 2*padding - filter_size)/straide + 1\n",
    "\n",
    "        return Nout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.network[W].shape =  (3, 2, 3)\n",
      "self.network[B].shape = (3,)\n"
     ]
    }
   ],
   "source": [
    "initializer = SimpleInitializer(sigma = 0.03)\n",
    "\n",
    "cls_Conv1d = Conv1d(F_cnt = w.shape[0], F_ch = w.shape[1], F_size = w.shape[2], optimizer = \"SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (1, 2, 4)\n",
      "★★★self.A.shape =  (1, 3, 2)\n",
      "★★★self.A.shape =  (1, 3, 2)\n",
      "★★★self.A =  [[[15. 21.]\n",
      "  [15. 21.]\n",
      "  [15. 21.]]]\n",
      "self.network[B] .shape =  (3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[16., 22.],\n",
       "        [17., 23.],\n",
       "        [18., 24.]]])"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = cls_Conv1d.forward(x, w, b)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜backward＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★★dA.shape =  (1, 3, 2)\n",
      "★★self.grad[W].shape =  (3, 2, 3)\n",
      "★★self.grad[B].shape =  (3,)\n",
      "★★self.X.shape =  (1, 2, 4)\n",
      "N_bat =  1\n",
      "F_cnt =  3\n",
      "ch_cnt =  2\n",
      "S_cnt =  3\n",
      "dA =  [[[16. 22.]\n",
      "  [17. 23.]\n",
      "  [18. 24.]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "delta_b, delta_w, delta_x = cls_Conv1d.backward(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([120.])"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 60.,  98., 136.],\n",
       "        [ 98., 136., 174.]],\n",
       "\n",
       "       [[ 63., 103., 143.],\n",
       "        [103., 143., 183.]],\n",
       "\n",
       "       [[ 66., 108., 150.],\n",
       "        [108., 150., 192.]]])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 51., 120., 120.,  69.],\n",
       "        [ 51., 120., 120.,  69.]]])"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜バッチあり＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(4,2,5) # shape(4, 2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.network[W].shape =  (3, 2, 3)\n",
      "self.network[B].shape = (3,)\n"
     ]
    }
   ],
   "source": [
    "cls_Conv1d_withB = Conv1d(F_cnt = 3, F_ch = 2, F_size = 3, optimizer = \"SGD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (4, 2, 5)\n",
      "★★★self.A.shape =  (4, 3, 3)\n",
      "★★★self.A.shape =  (4, 3, 3)\n",
      "★★★self.A =  [[[-3.11371392 -1.0653663   2.03707199]\n",
      "  [-3.11371392 -1.0653663   2.03707199]\n",
      "  [-3.11371392 -1.0653663   2.03707199]]\n",
      "\n",
      " [[-0.30124961 -0.85547559  0.92293406]\n",
      "  [-0.30124961 -0.85547559  0.92293406]\n",
      "  [-0.30124961 -0.85547559  0.92293406]]\n",
      "\n",
      " [[ 0.23446133 -2.6637255  -5.37410434]\n",
      "  [ 0.23446133 -2.6637255  -5.37410434]\n",
      "  [ 0.23446133 -2.6637255  -5.37410434]]\n",
      "\n",
      " [[ 1.97044708  1.96222513  0.02907767]\n",
      "  [ 1.97044708  1.96222513  0.02907767]\n",
      "  [ 1.97044708  1.96222513  0.02907767]]]\n",
      "self.network[B] .shape =  (3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-2.11371392, -0.0653663 ,  3.03707199],\n",
       "        [-1.11371392,  0.9346337 ,  4.03707199],\n",
       "        [-0.11371392,  1.9346337 ,  5.03707199]],\n",
       "\n",
       "       [[ 0.69875039,  0.14452441,  1.92293406],\n",
       "        [ 1.69875039,  1.14452441,  2.92293406],\n",
       "        [ 2.69875039,  2.14452441,  3.92293406]],\n",
       "\n",
       "       [[ 1.23446133, -1.6637255 , -4.37410434],\n",
       "        [ 2.23446133, -0.6637255 , -3.37410434],\n",
       "        [ 3.23446133,  0.3362745 , -2.37410434]],\n",
       "\n",
       "       [[ 2.97044708,  2.96222513,  1.02907767],\n",
       "        [ 3.97044708,  3.96222513,  2.02907767],\n",
       "        [ 4.97044708,  4.96222513,  3.02907767]]])"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = cls_Conv1d_withB.forward(x, w, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip【問題5】（アドバンス課題）パディングの実装\n",
    "畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "\n",
    "\n",
    "最も単純なパディングは全て0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "\n",
    "\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。\n",
    "\n",
    "\n",
    "numpy.pad — NumPy v1.17 Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】（アドバンス課題）ミニバッチへの対応\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⇒Scratch1dCNNClassifierで対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip【問題7】（アドバンス課題）任意のストライド数\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ＜FC＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        # 初期化        \n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.network = {}\n",
    "        self.network[\"W\"] = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.network[\"B\"] = initializer.B(n_nodes2)\n",
    "\n",
    "        self.A = 0\n",
    "        self.grad = {}\n",
    "        self.X = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "\n",
    "        # 自層への入力\n",
    "        A =  np.dot(X, self.network[\"W\"] ) + self.network[\"B\"]\n",
    "        self.X = X\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # bの勾配\n",
    "        self.grad[\"B\"] = np.sum(dA, axis=0)\n",
    "\n",
    "        # Wの勾配       \n",
    "        self.grad[\"W\"]= np.dot(self.X.T, dA)\n",
    "    \n",
    "        # 次層へのデルタ\n",
    "        dZ = np.dot(dA, self.network[\"W\"].T)\n",
    "    \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】学習と推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ＜Scratch1dCNNClassifier＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "class Scratch1dCNNClassifier:\n",
    "      \n",
    "\n",
    "    def __init__(self, lr, n_nodes = 780, n_output = 10, sigma = 0.03, epoch = 50, verbose = True, Initializer = \"simple\", optimizer = \"SGD\", activation = \"Relu\"):\n",
    "\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_output = n_output   \n",
    "        self.batch_size = 20 # バッチサイズ\n",
    "        self.epoch = epoch\n",
    "        self.train_loss_list = []\n",
    "        self.test_loss_list = []\n",
    "        self.verbose = verbose\n",
    "        self.sigma = sigma\n",
    "        self.Initializer = Initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        # 活性化関数のインスタンス化\n",
    "        if self.activation == \"Tanh\":\n",
    "            self.activation1 = Tanh()        \n",
    "            self.activation2 = Tanh()        \n",
    "            \n",
    "        elif self.activation == \"Sigmoid\":\n",
    "            self.activation1 = Sigmoid()        \n",
    "            self.activation2 = Sigmoid() \n",
    "            \n",
    "        elif self.activation == \"Relu\":\n",
    "            self.activation1 = Relu()        \n",
    "            self.activation2 = Relu()             \n",
    "            \n",
    "        self.activation3 = Softmax()        \n",
    "        \n",
    "        # 最適化手法のインスタンス化\n",
    "        if self.optimizer == \"SGD\":\n",
    "            optimizer1 = SGD(self.lr)   \n",
    "            optimizer2 = SGD(self.lr) \n",
    "            optimizer3 = SGD(self.lr) \n",
    "        elif self.optimizer == \"AdaGrad\":\n",
    "            optimizer1 = AdaGrad(self.lr)   \n",
    "            optimizer2 = AdaGrad(self.lr)  \n",
    "            optimizer3 = AdaGrad(self.lr)  \n",
    "        elif self.optimizer == \"Adam\":\n",
    "            optimizer1 = Adam(self.lr)   \n",
    "            optimizer2 = Adam(self.lr)  \n",
    "            optimizer3 = Adam(self.lr)  \n",
    "        \n",
    "        # 畳込み層のインスタンス化\n",
    "        self.Conv1 = Conv1d(1, X.shape[1], 3, optimizer1)   \n",
    "        self.Conv2 = Conv1d(1, X.shape[1], 3, optimizer2)   \n",
    "        \n",
    "        # 平滑化クラスのインスタンス化\n",
    "        self.Flattern =Flattern()        \n",
    "        \n",
    "        \n",
    "        # 全結合層のインスタンス化\n",
    "        if self.Initializer == \"simple\":\n",
    "            self.FC3 = FC(780, self.n_output, SimpleInitializer(self.sigma), optimizer3)\n",
    "        \n",
    "        elif self.Initializer == \"Xavier\":\n",
    "            self.FC3 = FC(780, self.n_output, XavierInitializer(self.sigma), optimizer3)\n",
    "        \n",
    "        elif self.Initializer == \"He\":\n",
    "            self.FC3 = FC(780, self.n_output, HeInitializer(self.sigma), optimizer3)\n",
    "\n",
    "        \n",
    "        print(\"Learning Start!\")\n",
    "        \n",
    "        # one-hot-vectol化\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "        y_train_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "        \n",
    "        if y_val is not None:\n",
    "            y_test_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "        \n",
    "        # 学習回数のカウンタ\n",
    "        learning_cnt = 0\n",
    "        \n",
    "        loss_func = loss_function()\n",
    "        \n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            # ミニバッチ取得\n",
    "            get_mini_batch = GetMiniBatch(X, y_train_one_hot, batch_size = self.batch_size)\n",
    "            \n",
    "            # すべてのミニバッチを抜ける前に直前の値をバックアップしておく(グラフ用に)\n",
    "            train_loss_batch = []\n",
    "            test_loss_batch = []\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # このfor文内でミニバッチが使える\n",
    "                z1, z2, z3 = self._forward(mini_X_train)                \n",
    "                \n",
    "                grad = self._backward(z3, mini_y_train)\n",
    "                \n",
    "                learning_cnt += 1\n",
    "                \n",
    "                # ミニバッチ内のロスを格納\n",
    "                train_loss_batch.append(loss_func.cross_entropy_error(mini_y_train, z3))\n",
    "     \n",
    "\n",
    "            # loss計算\n",
    "            # ミニバッチ内のロスの平均を取る\n",
    "            train_loss_mean = np.array(train_loss_batch).mean()\n",
    "            self.train_loss_list.append(train_loss_mean)\n",
    "\n",
    "            # test_loss の初期化\n",
    "            test_loss = 0\n",
    "            if X_val is not None:\n",
    "                z1, z2, y_test_pred = self._forward(X_val)\n",
    "                test_loss = loss_func.cross_entropy_error(y_test_one_hot, y_test_pred)\n",
    "                self.test_loss_list.append(test_loss)\n",
    "\n",
    "            # test_loss差分表示処理\n",
    "            if len(self.test_loss_list) == 1:\n",
    "                diff = 0\n",
    "            else:\n",
    "                diff = -1 * (self.test_loss_list[-2] - self.test_loss_list[-1])\n",
    "            \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "                print(\"Epoch Count = {}, train_loss = {:.5f}, test_loss = {:.5f}, diff = {}\".format(i+1, train_loss_mean, test_loss, diff))\n",
    "                \n",
    "        print(\"Learning Finish!\")\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        z1, z2, y_test_pred = self._forward(X)\n",
    "\n",
    "        y_pred = np.argmax(y_test_pred, axis = 1)\n",
    "        return y_pred\n",
    "    \n",
    "        \n",
    "        \n",
    "    # 順伝播\n",
    "    def _forward(self, X):\n",
    "        '''\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        '''\n",
    "        A1 = self.Conv1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.Conv2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        Z2 = self.Flattern.forward(Z2)    # Z2をそのまま平滑化\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        \n",
    "        return Z1, Z2, Z3\n",
    "        \n",
    "    # 誤差逆伝播\n",
    "    def _backward(self, Z3, Y):\n",
    "        dA3 = self.activation3.backward(Y, Z3) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "        dZ2 = self.FC3.backward(dA3)     \n",
    "        dZ2 = self.Flattern.backward(dZ2)    \n",
    "        dA2 = self.activation2.backward(dZ2)       \n",
    "        dZ1 = self.Conv2.backward(dA2)     \n",
    "        dA1 = self.activation1.backward(dZ1)     \n",
    "        dZ0 = self.Conv1.backward(dA1) # dZ0は使用しない\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flattern:\n",
    "    def __init__(self):\n",
    "        self.N = 1\n",
    "        self.C = 1\n",
    "        self.W = 1\n",
    "        \n",
    "        \n",
    "    def forward(self, X_in):\n",
    "        self.N, self.C, self.W = X_in.shape\n",
    "        X_out = X_in.reshape(self.N, self.C * self.W)\n",
    "        return X_out\n",
    "    \n",
    "    def backward(self, X_out):\n",
    "        N, Flat = X_out.shape\n",
    "        X_in = X_out.reshape(N, self.C, self.W)\n",
    "        return X_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid, Tanh, Relu, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    活性化関数：sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : 活性化関数への入力\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.Z = 0\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.Z = 1/(1 + np.exp(-A))\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dx = dZ * (1.0 - self.Z) * self.Z\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    活性化関数：sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : 活性化関数への入力\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.A = 0\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dx = dZ * (1.0 - self.forward(self.A)**2)\n",
    "        return dx\n",
    "    \n",
    "\n",
    "\n",
    "class Relu:\n",
    "    \"\"\"\n",
    "    活性化関数：relu\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : 活性化関数への入力\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.A = 0\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.maximum(0, A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, Z):\n",
    "        dx = np.where(self.A > 0, Z, 0)\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    活性化関数：softmax\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : 活性化関数への入力\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, A):\n",
    "        if A.ndim == 2:\n",
    "            A = A.T\n",
    "            A = A - np.max(A, axis=0)\n",
    "            y = np.exp(A) / np.sum(np.exp(A), axis=0)\n",
    "            return y.T\n",
    "\n",
    "        A = A - np.max(A) # オーバーフロー対策\n",
    "        return np.exp(A) / np.sum(np.exp(A))\n",
    "    \n",
    "    def backward(self, y, y_pred):\n",
    "        batch_size = y.shape[0]   \n",
    "\n",
    "        if y.size == y_pred.size: # 教師データがone-hot-vectorの場合\n",
    "            dx = (y_pred - y) / batch_size\n",
    "        else:\n",
    "            dx = y.copy()\n",
    "            test = np.arange(batch_size)\n",
    "            dx[np.arange(batch_size), y] -= 1\n",
    "            dx = dx / batch_size\n",
    "            \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss_function:\n",
    "# クロスエントロピー\n",
    "    def cross_entropy_error(self, d, y):\n",
    "        if y.ndim == 1:\n",
    "            d = d.reshape(1, d.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "\n",
    "        # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "        if d.size == y.size:\n",
    "            d = d.argmax(axis=1)\n",
    "\n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST読込\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 処理に時間のかかる場合はデータを削減 \n",
    "X_train, y_train = X_train[:5000], y_train[:5000]\n",
    "X_test, y_test = X_test[:1000], y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train /255\n",
    "X_test = X_test /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1chで平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1, 784)\n"
     ]
    }
   ],
   "source": [
    "# チャンネル方向の次元を追加　⇒　バッチ数、チャンネル、特徴量　へ\n",
    "X_train = X_train[:,np.newaxis, :]\n",
    "X_test = X_test[:,np.newaxis, :]\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ＜実行＞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scratch1dCNN = Scratch1dCNNClassifier(lr = 0.001, epoch= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Start!\n",
      "Epoch Count = 1, train_loss = 2.27196, test_loss = 2.21058, diff = 0\n",
      "Epoch Count = 2, train_loss = 2.13119, test_loss = 2.09266, diff = -0.11791660900051681\n",
      "Epoch Count = 3, train_loss = 2.01218, test_loss = 1.99069, diff = -0.10197617999562514\n",
      "Epoch Count = 4, train_loss = 1.90895, test_loss = 1.90252, diff = -0.08816724755863703\n",
      "Epoch Count = 5, train_loss = 1.81926, test_loss = 1.82597, diff = -0.07654835493638656\n",
      "Epoch Count = 6, train_loss = 1.74093, test_loss = 1.75906, diff = -0.06691059476132799\n",
      "Epoch Count = 7, train_loss = 1.67205, test_loss = 1.70011, diff = -0.0589457615833735\n",
      "Epoch Count = 8, train_loss = 1.61103, test_loss = 1.64777, diff = -0.0523459158838242\n",
      "Epoch Count = 9, train_loss = 1.55657, test_loss = 1.60092, diff = -0.046843836034869524\n",
      "Epoch Count = 10, train_loss = 1.50761, test_loss = 1.55870, diff = -0.04222160515893836\n",
      "Epoch Count = 11, train_loss = 1.46333, test_loss = 1.52040, diff = -0.03830629499272531\n",
      "Epoch Count = 12, train_loss = 1.42302, test_loss = 1.48543, diff = -0.034962075483699895\n",
      "Epoch Count = 13, train_loss = 1.38613, test_loss = 1.45335, diff = -0.03208243362490126\n",
      "Epoch Count = 14, train_loss = 1.35220, test_loss = 1.42377, diff = -0.029583618341632922\n",
      "Epoch Count = 15, train_loss = 1.32085, test_loss = 1.39637, diff = -0.027399444958730923\n",
      "Epoch Count = 16, train_loss = 1.29178, test_loss = 1.37089, diff = -0.025477282662159872\n",
      "Epoch Count = 17, train_loss = 1.26470, test_loss = 1.34712, diff = -0.02377498859427485\n",
      "Epoch Count = 18, train_loss = 1.23941, test_loss = 1.32486, diff = -0.02225857312546764\n",
      "Epoch Count = 19, train_loss = 1.21570, test_loss = 1.30396, diff = -0.020900420954240273\n",
      "Epoch Count = 20, train_loss = 1.19342, test_loss = 1.28428, diff = -0.019677931845590635\n",
      "Learning Finish!\n"
     ]
    }
   ],
   "source": [
    "Scratch1dCNN.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Scratch1dCNN.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.670213</td>\n",
       "      <td>0.762821</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.645455</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.567010</td>\n",
       "      <td>0.566929</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.637689</td>\n",
       "      <td>0.641747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.741176</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.645455</td>\n",
       "      <td>0.321839</td>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.662921</td>\n",
       "      <td>0.436170</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.628581</td>\n",
       "      <td>0.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.703911</td>\n",
       "      <td>0.843972</td>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.708134</td>\n",
       "      <td>0.645455</td>\n",
       "      <td>0.434109</td>\n",
       "      <td>0.597826</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.620769</td>\n",
       "      <td>0.628297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>0.638</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0           1           2           3           4  \\\n",
       "precision   0.670213    0.762821    0.571429    0.725490    0.645455   \n",
       "recall      0.741176    0.944444    0.482759    0.691589    0.645455   \n",
       "f1-score    0.703911    0.843972    0.523364    0.708134    0.645455   \n",
       "support    85.000000  126.000000  116.000000  107.000000  110.000000   \n",
       "\n",
       "                   5          6          7          8          9  accuracy  \\\n",
       "precision   0.666667   0.567010   0.566929   0.517544   0.683333     0.638   \n",
       "recall      0.321839   0.632184   0.727273   0.662921   0.436170     0.638   \n",
       "f1-score    0.434109   0.597826   0.637168   0.581281   0.532468     0.638   \n",
       "support    87.000000  87.000000  99.000000  89.000000  94.000000     0.638   \n",
       "\n",
       "             macro avg  weighted avg  \n",
       "precision     0.637689      0.641747  \n",
       "recall        0.628581      0.638000  \n",
       "f1-score      0.620769      0.628297  \n",
       "support    1000.000000   1000.000000  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# pandas.DataFrameへ変換\n",
    "df_cr = pd.DataFrame(cr)\n",
    "df_cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACC ≒ 64%  \n",
    "\n",
    "あまりよろしくない。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1958bac1d08>]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(len(Scratch1dCNN.train_loss_list)), Scratch1dCNN.train_loss_list, color = \"blue\")\n",
    "plt.plot(range(len(Scratch1dCNN.test_loss_list)), Scratch1dCNN.test_loss_list, color = \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まだロスが下がりそうだが、膨大な時間がかかる為、省略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次元の呪いは、あぁ～～恐ろしや😱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
